{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52ff269-9ceb-45d9-ba26-186314813d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando a sessão Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import unicodedata\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# Inicialização da SparkSession localmente\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkSession\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f34c1a-d6dd-424e-b447-911a12e1f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover acentos\n",
    "def remove_accents(inputStr):\n",
    "    if inputStr is not None:\n",
    "        return unicodedata.normalize('NFD', inputStr).encode('ascii', 'ignore').decode('utf-8')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Registrar a função como uma UDF\n",
    "remove_accents_udf = udf(remove_accents, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0baefaa7-0dac-4372-886f-01f64479b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento das siglas dos estados\n",
    "siglas_estados = {\n",
    "    \"Acre\": \"AC\", \"Alagoas\": \"AL\", \"Amapá\": \"AP\", \"Amazonas\": \"AM\", \"Bahia\": \"BA\", \"Ceará\": \"CE\",\n",
    "    \"Distrito Federal\": \"DF\", \"Espírito Santo\": \"ES\", \"Goiás\": \"GO\", \"Maranhão\": \"MA\", \"Mato Grosso\": \"MT\",\n",
    "    \"Mato Grosso do Sul\": \"MS\", \"Minas Gerais\": \"MG\", \"Pará\": \"PA\", \"Paraíba\": \"PB\", \"Paraná\": \"PR\",\n",
    "    \"Pernambuco\": \"PE\", \"Piauí\": \"PI\", \"Rio de Janeiro\": \"RJ\", \"Rio Grande do Norte\": \"RN\", \"Rio Grande do Sul\": \"RS\",\n",
    "    \"Rondônia\": \"RO\", \"Roraima\": \"RR\", \"Santa Catarina\": \"SC\", \"São Paulo\": \"SP\", \"Sergipe\": \"SE\", \"Tocantins\": \"TO\"\n",
    "}\n",
    "\n",
    "# Função UDF para substituir o nome do estado pela sigla\n",
    "def substituir_estado_por_sigla(estado_nome):\n",
    "    return siglas_estados.get(estado_nome, estado_nome)\n",
    "\n",
    "substituir_estado_por_sigla_udf = udf(substituir_estado_por_sigla, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2126142b-1b7d-4de8-83a1-ef4c343e5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquema esperado\n",
    "expected_schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),   \n",
    "    StructField(\"cpf\", StringType(), False),\n",
    "    StructField(\"nome_completo\", StringType(), False),\n",
    "    StructField(\"convenio\", StringType(), False),\n",
    "    StructField(\"data_nascimento\", DateType(), False),\n",
    "    StructField(\"sexo\", StringType(), False),\n",
    "    StructField(\"logradouro\", StringType(), False),\n",
    "    StructField(\"numero\", IntegerType(), False),\n",
    "    StructField(\"bairro\", StringType(), False),\n",
    "    StructField(\"cidade\", StringType(), False),\n",
    "    StructField(\"estado\", StringType(), False),\n",
    "    StructField(\"data_cadastro\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Validador de esquema e qualidade dos dados\n",
    "def validar_esquema(df, expected_schema):\n",
    "    actual_schema = df.schema\n",
    "    for expected_field, actual_field in zip(expected_schema, actual_schema):\n",
    "        if expected_field.name != actual_field.name or expected_field.dataType != actual_field.dataType:\n",
    "            raise ValueError(f\"Esquema não corresponde ao esperado.\\nEsperado: {expected_schema}\\nAtual: {actual_schema}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ff534e-b903-489e-b285-1e5a1ef6761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratar valores nulos\n",
    "def tratar_valores_nulos(df):\n",
    "    for field in df:\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(\"N/A\")).otherwise(col(field.name)))\n",
    "        elif isinstance(field.dataType, IntegerType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(0)).otherwise(col(field.name)))\n",
    "        elif isinstance(field.dataType, DateType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(\"1970-01-01\").cast(DateType())).otherwise(col(field.name)))\n",
    "        elif isinstance(field.dataType, TimestampType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(\"1970-01-01 00:00:00\").cast(TimestampType())).otherwise(col(field.name)))\n",
    "    return df\n",
    "\n",
    "# Remover CPFs duplicados e manter o registro com a data de cadastro mais recente\n",
    "def remover_cpfs_duplicados(df):\n",
    "    window_spec = Window.partitionBy(\"cpf\").orderBy(col(\"data_cadastro\").desc())\n",
    "    df = df.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "           .filter(col(\"row_number\") == 1) \\\n",
    "           .drop(\"row_number\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7408a27-e93d-4c2d-8ad3-ee7b317fc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para mascarar CPF\n",
    "def esconder_cpf(cpf):\n",
    "    if cpf is not None and len(cpf) == 11:\n",
    "        return \"********\" + cpf[-3:]\n",
    "    return cpf\n",
    "\n",
    "# Registrar a função como uma UDF\n",
    "esconder_cpf_udf = udf(esconder_cpf, StringType())\n",
    "\n",
    "# validação de dados com Great Expectations\n",
    "def validar_qualidade_dados_ge(df):\n",
    "    # Convertendo o DataFrame para um DataFrame do Great Expectations\n",
    "    df_ge = SparkDFDataset(df)\n",
    "\n",
    "    # Verificar se há valores nulos nas colunas obrigatórias\n",
    "    colunas_obrigatorias = [\"nome_completo\", \"convenio\"]\n",
    "    for coluna in colunas_obrigatorias:\n",
    "        expectation_result = df_ge.expect_column_values_to_not_be_null(coluna)\n",
    "        if not expectation_result[\"success\"]:\n",
    "            raise ValueError(f\"A coluna {coluna} contém valores nulos ou em branco.\")\n",
    "\n",
    "    # Verificar se o DataFrame possui registros\n",
    "    row_count = df_ge.count()\n",
    "    if row_count == 0:\n",
    "        raise ValueError(\"O DataFrame está vazio. Nenhum registro encontrado.\")\n",
    "\n",
    "    # Verificar se a coluna 'cpf' não é nula, tem exatamente 11 caracteres, é uma string com dígitos e é única\n",
    "    expectation_result = df_ge.expect_column_values_to_match_regex(\"cpf\", r\"^\\d{11}$\")\n",
    "    if not expectation_result[\"success\"]:\n",
    "        raise ValueError(f\"Existem CPFs com formato inválido ou que não são strings com 11 dígitos.\")\n",
    "\n",
    "    # Verificar se os valores de CPF são únicos\n",
    "    expectation_result = df_ge.expect_column_values_to_be_unique(\"cpf\")\n",
    "    if not expectation_result[\"success\"]:\n",
    "        raise ValueError(f\"Existem CPFs duplicados.\")\n",
    "\n",
    "    print(\"Todos os registros estão válidos.\")\n",
    "    \n",
    "    # Mascarar os CPFs\n",
    "    df = df.withColumn(\"cpf\", esconder_cpf_udf(col(\"cpf\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c37b6f-2095-4776-a5c8-ede28094bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler, transformar e padronizar dados do Parquet\n",
    "def transform_parquet(spark, file_path):\n",
    "    df_parquet = spark.read.parquet(file_path)\n",
    "    df_parquet_tratamento = df_parquet.alias(\"df_parquet_tratamento\")\n",
    "    df_transformado_parquet = df_parquet_tratamento.withColumnRenamed(\"documento_cpf\", \"cpf\") \\\n",
    "                                    .withColumn(\"cpf\", regexp_replace(col(\"cpf\"), \"[.-]\", \"\")) \\\n",
    "                                    .withColumn(\"nome_completo\", trim(regexp_replace(regexp_replace(col(\"nome_completo\"), \"Sr\\\\.|Sra\\\\.|Dr\\\\.|Srta\\\\.|Dra\\\\.\", \"\"), \"\\\\s+\", \" \"))) \\\n",
    "                                    .withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"yyyy-MM-dd\")) \\\n",
    "                                    .withColumn(\"data_cadastro\", to_timestamp(col(\"data_cadastro\"), \"yyyy-MM-dd'T'HH:mm:ss\")) \\\n",
    "                                    .withColumn(\"cidade\", initcap(col(\"cidade\"))) \\\n",
    "                                    .withColumn(\"sexo\", when(col(\"sexo\") == \"Fem\", \"F\").otherwise(when(col(\"sexo\") == \"Masc\", \"M\").otherwise(col(\"sexo\")))) \\\n",
    "                                    .withColumn(\"numero\", col(\"numero\").cast(IntegerType())) \\\n",
    "                                    .withColumnRenamed(\"uf\", \"estado\") \\\n",
    "                                    .drop(\"__index_level_0__\") \\\n",
    "                                    .drop(\"pais\")\n",
    "    for column_name in df_transformado_parquet.schema.names:\n",
    "        if isinstance(df_transformado_parquet.schema[column_name].dataType, StringType):\n",
    "            df_transformado_parquet = df_transformado_parquet.withColumn(column_name, remove_accents_udf(col(column_name)))\n",
    "    colunas_ordenadas = [\"cpf\", \"nome_completo\", \"convenio\" ,\"data_nascimento\", \"sexo\" , \"logradouro\" , \"numero\" , \"bairro\", \"cidade\" , \"estado\" , \"data_cadastro\"]\n",
    "    df_transformado_parquet = df_transformado_parquet.select(colunas_ordenadas)\n",
    "    return df_transformado_parquet\n",
    "\n",
    "# Função para ler, transformar e padronizar dados do CSV\n",
    "def transform_csv(spark, file_path):\n",
    "    df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"|\").load(file_path)\n",
    "    df_csv_tratamento = df_csv.alias(\"df_csv_tratamento\")\n",
    "    df_transformado_csv = df_csv_tratamento.withColumn(\"nome\", trim(regexp_replace(regexp_replace(col(\"nome\"), \"Sr\\\\.|Sra\\\\.|Dr\\\\.|Srta\\\\.|Dra\\\\.\", \"\"), \"\\\\s+\", \" \"))) \\\n",
    "                                    .withColumnRenamed(\"nome\", \"nome_completo\") \\\n",
    "                                    .withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"dd/MM/yyyy\")) \\\n",
    "                                    .withColumn(\"data_nascimento\", col(\"data_nascimento\").cast(\"date\")) \\\n",
    "                                    .withColumn(\"data_cadastro\", to_timestamp(col(\"data_cadastro\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "                                    .withColumn(\"cidade\", initcap(col(\"cidade\"))) \\\n",
    "                                    .withColumn(\"sexo\", when(col(\"sexo\") == \"Fem\", \"F\").otherwise(when(col(\"sexo\") == \"Masc\", \"M\").otherwise(col(\"sexo\")))) \\\n",
    "                                    .withColumn(\"numero\", regexp_replace(col(\"numero\"), \"[\\\\.0]\", \"\")) \\\n",
    "                                    .withColumn(\"numero\", col(\"numero\").cast(IntegerType())) \\\n",
    "                                    .drop(\"pais_cadastro\")\n",
    "    for column_name in df_transformado_csv.schema.names:\n",
    "        if isinstance(df_transformado_csv.schema[column_name].dataType, StringType):\n",
    "            df_transformado_csv = df_transformado_csv.withColumn(column_name, remove_accents_udf(col(column_name)))\n",
    "    colunas_ordenadas = [\"cpf\", \"nome_completo\", \"convenio\" ,\"data_nascimento\", \"sexo\" , \"logradouro\" , \"numero\" , \"bairro\", \"cidade\" , \"estado\" , \"data_cadastro\"]\n",
    "    df_transformado_csv = df_transformado_csv.select(colunas_ordenadas)\n",
    "    return df_transformado_csv\n",
    "\n",
    "# Função para ler, transformar e padronizar dados do JSON\n",
    "def transform_json(spark, file_path):\n",
    "    df_json = spark.read.json(file_path)\n",
    "    df_json_tratamento = df_json.alias(\"df_json_tratamento\")\n",
    "    df_transformado_json = df_json_tratamento\\\n",
    "        .withColumn(\"nome\", trim(regexp_replace(regexp_replace(col(\"nome\"), \"Sr\\\\.|Sra\\\\.|Dr\\\\.|Srta\\\\.|Dra\\\\.\", \"\"), \"\\\\s+\", \" \")))\\\n",
    "        .withColumnRenamed(\"nome\", \"nome_completo\")\\\n",
    "        .withColumn(\"cpf\", regexp_replace(col(\"cpf\"), \"[.-]\", \"\")) \\\n",
    "        .withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"MMMM dd, yyyy\"))\\\n",
    "        .withColumn(\"data_nascimento\", col(\"data_nascimento\").cast(\"date\"))\\\n",
    "        .withColumn(\"data_cadastro\", from_unixtime(col(\"data_cadastro\") / 1000).cast(\"timestamp\"))\\\n",
    "        .withColumn(\"cidade\", initcap(col(\"cidade\")))\\\n",
    "        .withColumn(\"numero\", col(\"numero\").cast(IntegerType()))\\\n",
    "        .withColumn(\"estado\", substituir_estado_por_sigla_udf(col(\"estado\")))\n",
    "    for column_name in df_transformado_json.schema.names:\n",
    "        if isinstance(df_transformado_json.schema[column_name].dataType, StringType):\n",
    "            df_transformado_json = df_transformado_json.withColumn(column_name, remove_accents_udf(col(column_name)))\n",
    "    colunas_ordenadas = [\"cpf\", \"nome_completo\", \"convenio\" ,\"data_nascimento\", \"sexo\" , \"logradouro\" , \"numero\" , \"bairro\", \"cidade\" , \"estado\" , \"data_cadastro\"]\n",
    "    df_transformado_json = df_transformado_json.select(colunas_ordenadas)\n",
    "    return df_transformado_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f1d69-1fbe-436d-b3c3-c23c5e60639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos dos arquivos locais ou do S3\n",
    "parquet_path = \"/home/jovyan/work/data/dados_cadastro_1.parquet\"\n",
    "csv_path = \"/home/jovyan/work/data/dados_cadastro_2.csv\"\n",
    "json_path = \"/home/jovyan/work/data/dados_cadastro_3.json\"\n",
    "\n",
    "# Transformar e padronizar dados\n",
    "df_transformado_parquet = transform_parquet(spark, parquet_path)\n",
    "df_transformado_csv = transform_csv(spark, csv_path)\n",
    "df_transformado_json = transform_json(spark, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beaf590e-7a62-4e7f-81f1-b8d704a64a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando os DataFrames\n",
    "tabela_unica = df_transformado_parquet.union(df_transformado_csv).union(df_transformado_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cce9667-d5fe-44db-9f9c-b7358ffb5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar valores nulos\n",
    "tabela_unica = tratar_valores_nulos(tabela_unica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c4d6326-9d72-4174-b094-10021562a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover CPFs duplicados\n",
    "tabela_unica = remover_cpfs_duplicados(tabela_unica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38ccee6e-cb6d-4d48-b6e7-3114db829f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cpf: string, nome_completo: string, convenio: string, data_nascimento: date, sexo: string, logradouro: string, numero: int, bairro: string, cidade: string, estado: string, data_cadastro: timestamp]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache do DataFrame antes da validação de dados\n",
    "tabela_unica.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a490f1-77fa-4580-8b2a-81dda8c802a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mascarar os CPFs\n",
    "tabela_unica = tabela_unica.withColumn(\"cpf\", esconder_cpf_udf(col(\"cpf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c831a394-2718-4fa8-8ffe-9d8a08a6642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar coluna de chave primária usando UUID\n",
    "tabela_unica = tabela_unica.withColumn(\"id\", expr(\"uuid()\"))\n",
    "\n",
    "# Reordenar as colunas para que 'id' seja a primeira coluna\n",
    "primeira_coluna = ['id']\n",
    "outras_colunas = [col for col in tabela_unica.columns if col != 'id']\n",
    "tabela_unica = tabela_unica.select(primeira_coluna + outras_colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a81aefa7-45e2-4906-bdec-c566bb3d84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar esquema final\n",
    "tabela_unica = validar_esquema(tabela_unica, expected_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50249103-8eee-4de7-8eea-23d4c57223af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------+---------------------+------------+---------------+----+--------------------+------+--------------------+------------------+------+-------------------+\n",
      "|id                                  |cpf        |nome_completo        |convenio    |data_nascimento|sexo|logradouro          |numero|bairro              |cidade            |estado|data_cadastro      |\n",
      "+------------------------------------+-----------+---------------------+------------+---------------+----+--------------------+------+--------------------+------------------+------+-------------------+\n",
      "|0f162b94-8cd1-4aa7-93d0-48ed3489982c|********752|Luiz Miguel Rodrigues|Porto Seguro|2017-06-04     |M   |Loteamento de Mendes|373   |Conjunto Serra Verde|Santos            |SP    |2014-04-02 04:22:42|\n",
      "|1afcab0d-4d06-43be-bcd0-156398cd5f4d|********634|Heitor Vieira        |Gold        |1978-05-28     |M   |Patio Vieira        |554   |Vila Havai          |Pires             |CE    |2001-09-18 18:26:08|\n",
      "|4922db69-2934-4660-8fb4-78de8da60627|********924|Alicia Fernandes     |Sulamerica  |2021-04-30     |F   |Recanto Oliveira    |584   |Vila Tirol          |Nogueira          |GO    |1972-05-23 01:40:13|\n",
      "|db7d54d8-631b-4031-8cfc-167474307335|********904|Milena Silva         |Bradesco    |2018-01-01     |F   |Estacao Agatha Lopes|469   |Nova Cintra         |Costela           |AC    |2016-02-11 18:21:54|\n",
      "|4bb1d7a5-baf0-490b-8650-b2aac5bdb9b9|********605|Maria Julia Moraes   |Prata       |2009-08-03     |F   |Colonia Santos      |44    |Vila Vista Alegre   |Novaes            |AL    |2014-09-17 21:43:09|\n",
      "|5aad6b59-7f75-43b1-ab9a-4d901ab28bc7|********788|Luna da Cruz         |Prata       |1971-11-29     |F   |Conjunto Ian Gomes  |3     |Vila Copacabana     |Cardoso Das Pedras|AP    |1996-09-03 20:28:34|\n",
      "|68d8015e-a21d-4304-8f89-7514e403c94f|********725|Maria Sophia Ferreira|Sulamerica  |1979-08-06     |F   |Rodovia de Vieira   |279   |Coqueiros           |Rocha             |AM    |2019-12-30 18:18:00|\n",
      "|0d95db39-ac84-4ebc-9d09-9ecae3f4f85f|********806|Isis da Paz          |Bradesco    |1974-10-24     |F   |Praia de da Mota    |706   |Vila Ouro Minas     |Pires             |PE    |1976-02-29 16:32:14|\n",
      "|d3aa5146-9019-4e4c-8f8b-988aa80eb79b|********698|Joao Gabriel Costa   |Sulamerica  |1995-09-19     |M   |Ladeira de da Cunha |981   |Jardim Guanabara    |Lopes Das Pedras  |RO    |2018-06-16 22:06:58|\n",
      "|beb96f51-2bba-45df-846e-2dda806b88a2|********880|Lucas Cavalcanti     |Prata       |1977-02-28     |M   |Avenida Barros      |242   |Vila Sao Geraldo    |Da Cunha          |MA    |1972-09-05 15:30:01|\n",
      "+------------------------------------+-----------+---------------------+------------+---------------+----+--------------------+------+--------------------+------------------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conferir o DataFrame resultante\n",
    "tabela_unica.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44fdfa83-696f-439e-a62a-242570340ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o DataFrame resultante no S3 particionado por convenio\n",
    "output_path = \"/home/jovyan/work/output/dados_tratados\"\n",
    "tabela_unica.write.partitionBy(\"convenio\").parquet(output_path, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
