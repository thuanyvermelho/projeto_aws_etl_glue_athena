{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52ff269-9ceb-45d9-ba26-186314813d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando a sessão Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import unicodedata\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# Inicialização da SparkSession localmente\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkSession\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0de6e7-cd48-45fe-90ff-f161cb327028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento das siglas dos estados\n",
    "siglas_estados = {\n",
    "    \"Acre\": \"AC\", \"Alagoas\": \"AL\", \"Amapá\": \"AP\", \"Amazonas\": \"AM\", \"Bahia\": \"BA\", \"Ceará\": \"CE\",\n",
    "    \"Distrito Federal\": \"DF\", \"Espírito Santo\": \"ES\", \"Goiás\": \"GO\", \"Maranhão\": \"MA\", \"Mato Grosso\": \"MT\",\n",
    "    \"Mato Grosso do Sul\": \"MS\", \"Minas Gerais\": \"MG\", \"Pará\": \"PA\", \"Paraíba\": \"PB\", \"Paraná\": \"PR\",\n",
    "    \"Pernambuco\": \"PE\", \"Piauí\": \"PI\", \"Rio de Janeiro\": \"RJ\", \"Rio Grande do Norte\": \"RN\", \"Rio Grande do Sul\": \"RS\",\n",
    "    \"Rondônia\": \"RO\", \"Roraima\": \"RR\", \"Santa Catarina\": \"SC\", \"São Paulo\": \"SP\", \"Sergipe\": \"SE\", \"Tocantins\": \"TO\"\n",
    "}\n",
    "\n",
    "# Função UDF para substituir o nome do estado pela sigla\n",
    "def substituir_estado_por_sigla(estado_nome):\n",
    "    return siglas_estados.get(estado_nome, estado_nome)\n",
    "\n",
    "substituir_estado_por_sigla_udf = udf(substituir_estado_por_sigla, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f34c1a-d6dd-424e-b447-911a12e1f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover acentos\n",
    "def remove_accents(inputStr):\n",
    "    if inputStr is not None:\n",
    "        return unicodedata.normalize('NFD', inputStr).encode('ascii', 'ignore').decode('utf-8')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Registrar a função como uma UDF\n",
    "remove_accents_udf = udf(remove_accents, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2126142b-1b7d-4de8-83a1-ef4c343e5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esquema esperado\n",
    "expected_schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),   \n",
    "    StructField(\"cpf\", StringType(), False),\n",
    "    StructField(\"nome_completo\", StringType(), False),\n",
    "    StructField(\"convenio\", StringType(), False),\n",
    "    StructField(\"data_nascimento\", DateType(), False),\n",
    "    StructField(\"sexo\", StringType(), False),\n",
    "    StructField(\"logradouro\", StringType(), False),\n",
    "    StructField(\"numero\", IntegerType(), False),\n",
    "    StructField(\"bairro\", StringType(), False),\n",
    "    StructField(\"cidade\", StringType(), False),\n",
    "    StructField(\"estado\", StringType(), False),\n",
    "    StructField(\"data_cadastro\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Validador de esquema e qualidade dos dados\n",
    "def validar_esquema(df, expected_schema):\n",
    "    actual_schema = df.schema\n",
    "    for expected_field, actual_field in zip(expected_schema, actual_schema):\n",
    "        if expected_field.name != actual_field.name or expected_field.dataType != actual_field.dataType:\n",
    "            raise ValueError(f\"Esquema não corresponde ao esperado.\\nEsperado: {expected_schema}\\nAtual: {actual_schema}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ff534e-b903-489e-b285-1e5a1ef6761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratar valores nulos\n",
    "def tratar_valores_nulos(df):\n",
    "    for field in df:\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(\"N/A\")).otherwise(col(field.name)))\n",
    "        elif isinstance(field.dataType, IntegerType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(0)).otherwise(col(field.name)))\n",
    "        elif isinstance(field.dataType, DateType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(\"1970-01-01\").cast(DateType())).otherwise(col(field.name)))\n",
    "        elif isinstance(field.dataType, TimestampType):\n",
    "            df = df.withColumn(field.name, when(col(field.name).isNull(), lit(\"1970-01-01 00:00:00\").cast(TimestampType())).otherwise(col(field.name)))\n",
    "    return df\n",
    "\n",
    "# Remover CPFs duplicados e manter o registro com a data de cadastro mais recente\n",
    "def remover_cpfs_duplicados(df):\n",
    "    window_spec = Window.partitionBy(\"cpf\").orderBy(col(\"data_cadastro\").desc())\n",
    "    df = df.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "           .filter(col(\"row_number\") == 1) \\\n",
    "           .drop(\"row_number\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7408a27-e93d-4c2d-8ad3-ee7b317fc8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validação de dados \n",
    "def validar_qualidade_dados(df):\n",
    "    # Verificar se há valores nulos nas colunas obrigatórias\n",
    "    colunas_obrigatorias = [\"nome_completo\", \"convenio\"]\n",
    "    colunas_com_nulos = [coluna for coluna in colunas_obrigatorias if df.filter(col(coluna).isNull() | (trim(col(coluna)) == \"\")).count() > 0]\n",
    "    if colunas_com_nulos:\n",
    "        raise ValueError(f\"As seguintes colunas contêm valores nulos ou em branco: {', '.join(colunas_com_nulos)}\")\n",
    "\n",
    "    # Verificar se o DataFrame possui registros\n",
    "    if df.count() == 0:\n",
    "        raise ValueError(\"O DataFrame está vazio. Nenhum registro encontrado.\")\n",
    "\n",
    "    # Verificar se a coluna 'cpf' não é nula, tem exatamente 11 caracteres, é uma string com dígitos e é única\n",
    "    df_cpf_invalido = df.filter(col(\"cpf\").isNull() | (length(col(\"cpf\")) != 11) | (~col(\"cpf\").rlike(\"^\\d{11}$\")))\n",
    "    if df_cpf_invalido.count() > 0:\n",
    "        raise ValueError(f\"Existem {df_cpf_invalido.count()} registros com CPF nulo, com formato inválido ou que não são strings com 11 dígitos.\")\n",
    "\n",
    "    # Verificar se os valores de CPF são únicos\n",
    "    df_cpf_duplicado = df.groupBy(\"cpf\").count().filter(col(\"count\") > 1)\n",
    "    if df_cpf_duplicado.count() > 0:\n",
    "        raise ValueError(f\"Existem {df_cpf_duplicado.count()} CPFs duplicados.\")\n",
    "\n",
    "    print(\"Todos os registros estão válidos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c37b6f-2095-4776-a5c8-ede28094bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler, transformar e padronizar dados do Parquet\n",
    "def transform_parquet(spark, file_path):\n",
    "    df_parquet = spark.read.parquet(file_path)\n",
    "    df_parquet_tratamento = df_parquet.alias(\"df_parquet_tratamento\")\n",
    "    df_transformado_parquet = df_parquet_tratamento.withColumnRenamed(\"documento_cpf\", \"cpf\") \\\n",
    "                                    .withColumn(\"cpf\", regexp_replace(col(\"cpf\"), \"[.-]\", \"\")) \\\n",
    "                                    .withColumn(\"nome_completo\", trim(regexp_replace(regexp_replace(col(\"nome_completo\"), \"Sr\\\\.|Sra\\\\.|Dr\\\\.|Srta\\\\.|Dra\\\\.\", \"\"), \"\\\\s+\", \" \"))) \\\n",
    "                                    .withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"yyyy-MM-dd\")) \\\n",
    "                                    .withColumn(\"data_cadastro\", to_timestamp(col(\"data_cadastro\"), \"yyyy-MM-dd'T'HH:mm:ss\")) \\\n",
    "                                    .withColumn(\"cidade\", initcap(col(\"cidade\"))) \\\n",
    "                                    .withColumn(\"sexo\", when(col(\"sexo\") == \"Fem\", \"F\").otherwise(when(col(\"sexo\") == \"Masc\", \"M\").otherwise(col(\"sexo\")))) \\\n",
    "                                    .withColumn(\"numero\", col(\"numero\").cast(IntegerType())) \\\n",
    "                                    .withColumnRenamed(\"uf\", \"estado\") \\\n",
    "                                    .drop(\"__index_level_0__\") \\\n",
    "                                    .drop(\"pais\")\n",
    "    for column_name in df_transformado_parquet.schema.names:\n",
    "        if isinstance(df_transformado_parquet.schema[column_name].dataType, StringType):\n",
    "            df_transformado_parquet = df_transformado_parquet.withColumn(column_name, remove_accents_udf(col(column_name)))\n",
    "    colunas_ordenadas = [\"cpf\", \"nome_completo\", \"convenio\" ,\"data_nascimento\", \"sexo\" , \"logradouro\" , \"numero\" , \"bairro\", \"cidade\" , \"estado\" , \"data_cadastro\"]\n",
    "    df_transformado_parquet = df_transformado_parquet.select(colunas_ordenadas)\n",
    "    return df_transformado_parquet\n",
    "\n",
    "# Função para ler, transformar e padronizar dados do CSV\n",
    "def transform_csv(spark, file_path):\n",
    "    df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"|\").load(file_path)\n",
    "    df_csv_tratamento = df_csv.alias(\"df_csv_tratamento\")\n",
    "    df_transformado_csv = df_csv_tratamento.withColumn(\"nome\", trim(regexp_replace(regexp_replace(col(\"nome\"), \"Sr\\\\.|Sra\\\\.|Dr\\\\.|Srta\\\\.|Dra\\\\.\", \"\"), \"\\\\s+\", \" \"))) \\\n",
    "                                    .withColumnRenamed(\"nome\", \"nome_completo\") \\\n",
    "                                    .withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"dd/MM/yyyy\")) \\\n",
    "                                    .withColumn(\"data_nascimento\", col(\"data_nascimento\").cast(\"date\")) \\\n",
    "                                    .withColumn(\"data_cadastro\", to_timestamp(col(\"data_cadastro\"), \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "                                    .withColumn(\"cidade\", initcap(col(\"cidade\"))) \\\n",
    "                                    .withColumn(\"sexo\", when(col(\"sexo\") == \"Fem\", \"F\").otherwise(when(col(\"sexo\") == \"Masc\", \"M\").otherwise(col(\"sexo\")))) \\\n",
    "                                    .withColumn(\"numero\", regexp_replace(col(\"numero\"), \"[\\\\.0]\", \"\")) \\\n",
    "                                    .withColumn(\"numero\", col(\"numero\").cast(IntegerType())) \\\n",
    "                                    .drop(\"pais_cadastro\")\n",
    "    for column_name in df_transformado_csv.schema.names:\n",
    "        if isinstance(df_transformado_csv.schema[column_name].dataType, StringType):\n",
    "            df_transformado_csv = df_transformado_csv.withColumn(column_name, remove_accents_udf(col(column_name)))\n",
    "    colunas_ordenadas = [\"cpf\", \"nome_completo\", \"convenio\" ,\"data_nascimento\", \"sexo\" , \"logradouro\" , \"numero\" , \"bairro\", \"cidade\" , \"estado\" , \"data_cadastro\"]\n",
    "    df_transformado_csv = df_transformado_csv.select(colunas_ordenadas)\n",
    "    return df_transformado_csv\n",
    "\n",
    "# Função para ler, transformar e padronizar dados do JSON\n",
    "def transform_json(spark, file_path):\n",
    "    df_json = spark.read.json(file_path)\n",
    "    df_json_tratamento = df_json.alias(\"df_json_tratamento\")\n",
    "    df_transformado_json = df_json_tratamento\\\n",
    "        .withColumn(\"nome\", trim(regexp_replace(regexp_replace(col(\"nome\"), \"Sr\\\\.|Sra\\\\.|Dr\\\\.|Srta\\\\.|Dra\\\\.\", \"\"), \"\\\\s+\", \" \")))\\\n",
    "        .withColumnRenamed(\"nome\", \"nome_completo\")\\\n",
    "        .withColumn(\"cpf\", regexp_replace(col(\"cpf\"), \"[.-]\", \"\")) \\\n",
    "        .withColumn(\"data_nascimento\", to_date(col(\"data_nascimento\"), \"MMMM dd, yyyy\"))\\\n",
    "        .withColumn(\"data_nascimento\", col(\"data_nascimento\").cast(\"date\"))\\\n",
    "        .withColumn(\"data_cadastro\", from_unixtime(col(\"data_cadastro\") / 1000).cast(\"timestamp\"))\\\n",
    "        .withColumn(\"cidade\", initcap(col(\"cidade\")))\\\n",
    "        .withColumn(\"numero\", col(\"numero\").cast(IntegerType()))\\\n",
    "        .withColumn(\"estado\", substituir_estado_por_sigla_udf(col(\"estado\")))\n",
    "    for column_name in df_transformado_json.schema.names:\n",
    "        if isinstance(df_transformado_json.schema[column_name].dataType, StringType):\n",
    "            df_transformado_json = df_transformado_json.withColumn(column_name, remove_accents_udf(col(column_name)))\n",
    "    colunas_ordenadas = [\"cpf\", \"nome_completo\", \"convenio\" ,\"data_nascimento\", \"sexo\" , \"logradouro\" , \"numero\" , \"bairro\", \"cidade\" , \"estado\" , \"data_cadastro\"]\n",
    "    df_transformado_json = df_transformado_json.select(colunas_ordenadas)\n",
    "    return df_transformado_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f1d69-1fbe-436d-b3c3-c23c5e60639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos dos arquivos locais ou do S3\n",
    "parquet_path = \"/home/jovyan/work/data/dados_cadastro_1.parquet\"\n",
    "csv_path = \"/home/jovyan/work/data/dados_cadastro_2.csv\"\n",
    "json_path = \"/home/jovyan/work/data/dados_cadastro_3.json\"\n",
    "\n",
    "# Transformar e padronizar dados\n",
    "df_transformado_parquet = transform_parquet(spark, parquet_path)\n",
    "df_transformado_csv = transform_csv(spark, csv_path)\n",
    "df_transformado_json = transform_json(spark, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beaf590e-7a62-4e7f-81f1-b8d704a64a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando os DataFrames\n",
    "tabela_unica = df_transformado_parquet.union(df_transformado_csv).union(df_transformado_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cce9667-d5fe-44db-9f9c-b7358ffb5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar valores nulos\n",
    "tabela_unica = tratar_valores_nulos(tabela_unica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c4d6326-9d72-4174-b094-10021562a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover CPFs duplicados\n",
    "tabela_unica = remover_cpfs_duplicados(tabela_unica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c23bb8e-59eb-4361-acaa-0a5b2287b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os registros estão válidos.\n"
     ]
    }
   ],
   "source": [
    "# Executar qualidade de dados\n",
    "validar_qualidade_dados(tabela_unica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e52d65-dc7f-4da7-b1f6-adfbcaae8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando LGPD na tabela cadastro\n",
    "tabela_unica = (tabela_unica\n",
    "    .withColumn(\"logradouro\", repeat(lit(\"*\"), 10))\n",
    "    .withColumn(\"bairro\", repeat(lit(\"*\"), 10))\n",
    "    .withColumn(\"cidade\", repeat(lit(\"*\"), 10))\n",
    "    .withColumn(\"cpf\", repeat(lit(\"*\"), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c831a394-2718-4fa8-8ffe-9d8a08a6642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar coluna de chave primária usando UUID\n",
    "tabela_unica = tabela_unica.withColumn(\"id\", expr(\"uuid()\"))\n",
    "\n",
    "# Reordenar as colunas para que 'id' seja a primeira coluna\n",
    "primeira_coluna = ['id']\n",
    "outras_colunas = [col for col in tabela_unica.columns if col != 'id']\n",
    "tabela_unica = tabela_unica.select(primeira_coluna + outras_colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a81aefa7-45e2-4906-bdec-c566bb3d84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar esquema final\n",
    "tabela_unica = validar_esquema(tabela_unica, expected_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50249103-8eee-4de7-8eea-23d4c57223af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+------------------+----------+---------------+----+----------+------+----------+----------+------+-------------------+\n",
      "|id                                  |cpf       |nome_completo     |convenio  |data_nascimento|sexo|logradouro|numero|bairro    |cidade    |estado|data_cadastro      |\n",
      "+------------------------------------+----------+------------------+----------+---------------+----+----------+------+----------+----------+------+-------------------+\n",
      "|0b94380d-b6aa-4629-9ba9-183bf5006311|**********|Maria Julia Moraes|Prata     |2009-08-03     |F   |**********|44    |**********|**********|AL    |2014-09-17 21:43:09|\n",
      "|baeb60db-a42d-45a9-b4fe-e955df5b51d7|**********|Joao Gabriel Costa|Sulamerica|1995-09-19     |M   |**********|981   |**********|**********|RO    |2018-06-16 22:06:58|\n",
      "|6ae2c5aa-5540-421d-80e7-a874eb793b9c|**********|Thiago da Cruz    |Bradesco  |2001-06-14     |M   |**********|117   |**********|**********|MS    |1978-09-18 22:13:53|\n",
      "|f40544b6-d144-472c-96d5-1d376a776ac4|**********|Davi Lucas da Cruz|Prata     |2014-12-20     |M   |**********|354   |**********|**********|MG    |2010-09-22 10:49:37|\n",
      "|2c5a5a82-0c1a-41e5-8904-87e8cc268c9f|**********|Maria Clara Costa |Itau      |1981-11-22     |F   |**********|166   |**********|**********|ES    |1988-07-29 06:03:38|\n",
      "+------------------------------------+----------+------------------+----------+---------------+----+----------+------+----------+----------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conferir o DataFrame resultante\n",
    "tabela_unica.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8011ce6f-5217-46e4-aae3-55c9e13c0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quantidade de registros na tabela final é: 976.954\n"
     ]
    }
   ],
   "source": [
    "# Contando o número de registros no DataFrame\n",
    "print(f\"A quantidade de registros na tabela final é: {tabela_unica.count():,}\".replace(',', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44fdfa83-696f-439e-a62a-242570340ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o DataFrame resultante particionado por convenio\n",
    "output_path = \"/home/jovyan/work/output/dados_tratados\"\n",
    "tabela_unica.write.partitionBy(\"convenio\").parquet(output_path, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
